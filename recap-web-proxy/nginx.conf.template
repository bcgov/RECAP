worker_processes auto;
error_log  /var/log/nginx/error.log;
pid /tmp/nginx.pid;

events {
  worker_connections 4096;
}

http {
  include       /etc/nginx/mime.types;
  default_type  application/octet-stream;
  underscores_in_headers on;

  # Upstream configuration for private endpoint with connection management
  upstream azure_openai_backend {
    server {{PRIVATE_ENDPOINT_IP}}:443;
    keepalive 32;
    keepalive_requests 1000;
    keepalive_timeout 60s;
  }

  server {
    listen 80;
    server_name _;

    # Route Azure OpenAI API through private endpoint  
    location /openai/ {      
      # Forward the full path as-is to the backend using upstream
      proxy_pass https://azure_openai_backend;
      proxy_set_header Host {{OPENAI_SERVICE_HOST}};
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
      proxy_set_header Connection "";
      
      # Azure OpenAI Authentication - pass api-key header directly
      proxy_pass_header api-key;
      
      # Preserve original request headers
      proxy_pass_request_headers on;
      
      # SSL Configuration for private endpoint
      proxy_ssl_server_name on;
      proxy_ssl_name {{OPENAI_SERVICE_HOST}};
      proxy_ssl_protocols TLSv1.2 TLSv1.3;
      proxy_ssl_verify off;
      proxy_ssl_session_reuse on;
      
      # Connection management to prevent private endpoint issues
      proxy_http_version 1.1;
      
      # Add timeout and retry configuration
      proxy_connect_timeout 30s;
      proxy_send_timeout 30s;
      proxy_read_timeout 60s;
      
      # Expose token usage headers in response
      add_header Access-Control-Expose-Headers "x-ratelimit-remaining-tokens,x-ratelimit-remaining-requests,openai-processing-ms" always;
    }

    # Test endpoint to verify proxy is working
    location /api/test {
      return 200 "RECAP Proxy is working! Azure OpenAI endpoint: {{OPENAI_SERVICE_HOST}}\n";
      add_header Content-Type text/plain;
    }

    # Debug endpoint to verify openai location is working
    location /openai/debug {
      return 200 "OpenAI location is working! This confirms /openai/ routing is active.\n";
      add_header Content-Type text/plain;
    }

    location / {
      root   /usr/share/nginx/html;
      index  index.html;
      try_files $uri $uri/ /index.html;
    }
  }
}